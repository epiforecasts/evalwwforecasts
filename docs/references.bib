@Manual{wwinference,
  title = {wwinference: Jointly infers infection dynamics from wastewater data and
epidemiological indicators},
  author = {Kaitlyn Johnson and Dylan Morris and Sam Abbott and Christian {Bernal Zelaya} and George {Vega Yon} and Damon Bayer and Andrew Magee and Scott Olesen},
  year = {2025},
  note = {R package version 0.1.1.99, commit 85a6d5c57e6f4c6450cd637750fa58e036a8c0dd},
  url = {https://github.com/CDCgov/ww-inference-model},
}

@article {Sherratt2025,
	author = {Sherratt, Katharine and Grah, Rok and Prasse, Bastian and Becker, Friederike and McLean, Jamie and Abbott, Sam and Funk, Sebastian},
	title = {The influence of model structure and geographic specificity on predictive accuracy among European COVID-19 forecasts},
	elocation-id = {2025.04.10.25325611},
	year = {2025},
	doi = {10.1101/2025.04.10.25325611},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Modellers take many approaches to predicting the course of infectious diseases, and achieve a wide range of accuracy in resulting forecasting performance. For example, forecasters vary in their use of different underlying model structures, and in the extent to which they adapt a model to the specific forecast target. However, it has been difficult to evaluate the impact of these choices on subsequent forecast performance. Such evaluations need a comparable sample of forecasting models, while also accounting for varying predictive difficulty among multiple forecast targets. Here, we develop a model-based approach to start addressing these challenges. We apply this to a multi-country multi-model forecasting effort conducted during the COVID-19 pandemic, in order to assess the influence of models{\textquoteright} structure and specificity to the epidemic target on forecast accuracy.We evaluated 181,851 probabilistic predictions from 47 forecasting models participating in the European COVID-19 Forecast Hub between 8 March 2021 and 10 March 2023, classified by model structure (agent-based, mechanistic, semi-mechanistic, statistical, other); and specificity (the model produced forecasts for either one or multiple locations). We assessed performance of COVID-19 case and death forecasts, measured as the weighted interval score after log-transforming both forecasts and observations. We summarised performance descriptively and compared this to estimates from a generalised additive mixed effects model. We included adjustment for variation between countries over time, the epidemiological situation, the forecast horizon, and among models.Whilst unadjusted estimates pointed to differences in predictive performance between model structures, after adjustment there was little systematic difference in average performance. Models forecasting for only a single geographic target outperformed those that made predictions for multiple targets, although this was a weak signal. We noted substantial variation in model performance that our approach did not account for.Understanding the reasons behind forecast performance is useful for prioritising and interpreting modelling work. We showed that valid comparisons of forecast performance depend on appropriately adjusting for the general predictive difficulty of the target. This work was limited by a small sample size of independent models and likely incomplete adjustment for interactions and confounders influencing predictive difficulty. We recommend that multi-model comparisons encourage and document their methodological diversity to enable future studies of underlying factors driving predictive performance.Author summary Accurately predicting the spread of infectious disease is essential to supporting public health during outbreaks. However, comparing the accuracy of different forecasting models is challenging. Existing evaluations struggle to isolate the impact of model design choices (like model structure or specificity to the forecast target) from the inherent difficulty of predicting complex outbreak dynamics. Our research introduces a novel approach to address this by systematically adjusting for common factors affecting epidemiological forecasts, accounting for multi-layered and non-linear effects on predictive difficulty. We applied this approach to a large dataset of forecasts from 47 different models submitted to the European COVID-19 Forecast Hub. We adjusted for variation across epidemic dynamics, forecast horizon, location, time, and model-specific effects. This allowed us to isolate the impact of model structure and geographic specificity on predictive performance. Our findings suggest that after adjustment, apparent differences in performance between model structures became minimal, while models that were specific to a single location showed a slight performance advantage over multi-location models. Our work highlights the importance of considering predictive difficulty when evaluating across forecasting models, and provides a framework for more robust evaluations of infectious disease predictions.Competing Interest StatementThe authors have declared no competing interest.Funding StatementKS, SF, SA were funded by Wellcome Trust (grant number 200901/Z/16/Z).Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:Source data were openly available before the initiation of the study. Forecast and observed data were sourced from the European COVID-19 Forecast Hub, available to view at ˂https://covid19forecasthub.eu/˃. All Hub data are now archived at Github: ˂https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive˃ and Zenodo with DOI: ˂https://doi.org/10.5281/zenodo.13986751˃. Specific data used in this work are available in the Github repository for this paper at: ˂https://github.com/epiforecasts/eval-by-method/tree/main/data˃I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesThe codebase for this paper is publicly available at the Github repository {\textquotedblleft}epiforecasts/eval-by-method{\textquotedblright} and Zenodo with DOI: 10.5281/zenodo.14903162. Forecast and observed data were sourced from the European COVID-19 Forecast Hub, available to view online. All Hub data are now archived at the Github repository: {\textquotedblleft}european-modelling-hubs/covid19-forecast-hub-europe_archive{\textquotedblright} and Zenodo with DOI: 10.5281/zenodo.13986751. Specific data used in this work are available in the Github repository for this paper. https://github.com/epiforecasts/eval-by-method https://covid19forecasthub.eu/ https://github.com/european-modelling-hubs/covid19-forecast-hub-europe_archive https://doi.org/10.5281/zenodo.13986751},
	URL = {https://www.medrxiv.org/content/early/2025/04/11/2025.04.10.25325611},
	eprint = {https://www.medrxiv.org/content/early/2025/04/11/2025.04.10.25325611.full.pdf},
	journal = {medRxiv}
}

@article{Wolffram2023,
    doi = {10.1371/journal.pcbi.1011394},
    author = {Wolffram, Daniel AND Abbott, Sam AND an der Heiden, Matthias AND Funk, Sebastian AND Günther, Felix AND Hailer, Davide AND Heyder, Stefan AND Hotz, Thomas AND van de Kassteele, Jan AND Küchenhoff, Helmut AND Müller-Hansen, Sören AND Syliqi, Diellë AND Ullrich, Alexander AND Weigert, Maximilian AND Schienle, Melanie AND Bracher, Johannes},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Collaborative nowcasting of COVID-19 hospitalization incidences in Germany},
    year = {2023},
    month = {08},
    volume = {19},
    url = {https://doi.org/10.1371/journal.pcbi.1011394},
    pages = {1-25},
    abstract = {Real-time surveillance is a crucial element in the response to infectious disease outbreaks. However, the interpretation of incidence data is often hampered by delays occurring at various stages of data gathering and reporting. As a result, recent values are biased downward, which obscures current trends. Statistical nowcasting techniques can be employed to correct these biases, allowing for accurate characterization of recent developments and thus enhancing situational awareness. In this paper, we present a preregistered real-time assessment of eight nowcasting approaches, applied by independent research teams to German 7-day hospitalization incidences during the COVID-19 pandemic. This indicator played an important role in the management of the outbreak in Germany and was linked to levels of non-pharmaceutical interventions via certain thresholds. Due to its definition, in which hospitalization counts are aggregated by the date of case report rather than admission, German hospitalization incidences are particularly affected by delays and can take several weeks or months to fully stabilize. For this study, all methods were applied from 22 November 2021 to 29 April 2022, with probabilistic nowcasts produced each day for the current and 28 preceding days. Nowcasts at the national, state, and age-group levels were collected in the form of quantiles in a public repository and displayed in a dashboard. Moreover, a mean and a median ensemble nowcast were generated. We find that overall, the compared methods were able to remove a large part of the biases introduced by delays. Most participating teams underestimated the importance of very long delays, though, resulting in nowcasts with a slight downward bias. The accompanying prediction intervals were also too narrow for almost all methods. Averaged over all nowcast horizons, the best performance was achieved by a model using case incidences as a covariate and taking into account longer delays than the other approaches. For the most recent days, which are often considered the most relevant in practice, a mean ensemble of the submitted nowcasts performed best. We conclude by providing some lessons learned on the definition of nowcasting targets and practical challenges.},
    number = {8},

}
